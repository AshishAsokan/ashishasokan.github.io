<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<head>
    <title>NeRFactor</title>
    <link rel="stylesheet" href="./style.css">
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link rel="icon" type="image/png" href="assets/logos/csail.ico">
    <!-- Facebook -->
    <meta property="og:image" content="assets/images/cover.jpg">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://people.csail.mit.edu/xiuming/projects/nerfactor/"/>
    <meta property="og:title" content="NeRFactor" />
    <meta property="og:description" content="Project page for NeRFactor." />
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="NeRFactor" />
    <meta name="twitter:description" content="Project page for NeRFactor." />
    <meta name="twitter:image" content="assets/images/cover.jpg" />
</head>

<body>
    <h1><b>NeRFactor</b>: Neural Factorization of Shape and Reflectance<br/>Under an Unknown Illumination</h1>
    <div class="venue_card">
        <p class="venue">arXiv</p>
    </div>

    <div class="author_card">
        <p class="author">
            <a target='_blank' href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>&nbsp;<sup>1</sup>&emsp;&emsp;
            <a target='_blank' href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>&nbsp;<sup>2</sup>&emsp;&emsp;
            <a target='_blank' href="https://boyangdeng.com/">Boyang Deng</a>&nbsp;<sup>2</sup>&emsp;&emsp;
            <a target='_blank' href="http://www.pauldebevec.com/">Paul Debevec</a>&nbsp;<sup>2</sup>&emsp;&emsp;
            <a target='_blank' href="http://billf.mit.edu/">William T. Freeman</a>&nbsp;<sup>1,&nbsp;2</sup>&emsp;&emsp;
            <a target='_blank' href="https://jonbarron.info/">Jonathan T. Barron</a>&nbsp;<sup>2</sup>
        </p>
        <br>
        <table class="authoraffliation">
            <tr>
                <td><sup>1</sup>&nbsp;<a target='_blank' href="https://www.csail.mit.edu/">MIT CSAIL</a></td>
                <td><sup>2</sup>&nbsp;<a target='_blank' href="https://research.google/">Google Research</a></td>
            </tr>
            <tr>
                <td><img src="assets/logos/mit.png" height="40em">&emsp;<img src="assets/logos/csail.png" height="40em"></td>
                <td><img src="assets/logos/google-research.png" height="40em"></td>
            </tr>
        </table>
        <br>
    </div>

    <div class="card">
        <table>
            <tr>
                <h3>Abstract</h3>
                <p>We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks.</p>
            </tr>
            <tr>
                <img src="assets/images/teaser.jpg" width="100%">
                <p class="caption">Given a set of posed images of an object captured from multiple views under just one unknown illumination condition (left), Neural Radiance Factorization (NeRFactor) is able to factorize the scene into 3D neural fields of surface normals, light visibility, albedo, and material (center), which enables applications such as free-viewpoint relighting and material editing (right).</p>
            </tr>
        </table>
    </div>

    <div class="card">
        <h3>Paper</h3>
        <p class="pub">
            <b>NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination</b><br>
            <a target='_blank' href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
            <a target='_blank' href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
            <a target='_blank' href="https://boyangdeng.com/">Boyang Deng</a>,
            <a target='_blank' href="http://www.pauldebevec.com/">Paul Debevec</a>,
            <a target='_blank' href="http://billf.mit.edu/">William T. Freeman</a>,
            <a target='_blank' href="https://jonbarron.info/">Jonathan T. Barron</a>
            <br>
            <b>arXiv</b>
            <br>
            <a target='_blank' href="https://arxiv.org/pdf/2106.01970.pdf">arXiv</a>
        </p>
        <!--
        BibTeX
        <pre>
</pre>
        -->
        <p class="caption"><i>v0 (06/01/2020)</i></p>
    </div>

    <div class="card">
        <h3>Video</h3>
        <p>
            Direct downloads:&emsp;
            <a target='_blank' href="https://drive.google.com/file/d/1nPLhzUh7byjDlKOBKvaaeCGGGKlND3OL/view?usp=sharing">1080p (212 MB)</a>&ensp; / &ensp;
            <a target='_blank' href="https://drive.google.com/file/d/1FmhKl-BaJQHAlQiw8HO_KGo_OMuaCOqf/view?usp=sharing">720p (154 MB)</a>
        </p>
        <div style="position:relative;padding-top:56.25%;">
            <iframe src="https://www.youtube.com/embed/UUVSPJlwhPg" frameborder="0" allow="autoplay" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
        </div>
        <br>
        <p class="caption"><i>v0 (06/01/2020)</i></p>
    </div>

    <div class="card">
        <h3>Code</h3>
        <p>
            <a target='_blank' href="https://github.com/google/nerfactor">This GitHub repo.</a> includes code for:
            <div style="margin-left:2em;">
                <ul>
                    <li>Data rendering and processing (so that you can render your own scenes or process your own captures);
                    <li>Model training and validation (so that you can modify the model and train it on your own data); and</li>
                    <li>Model inference/testing (so that you can quickly try out our trained models).</li>
                </ul>
            </div>
        </p>
    </div>

    <div class="card">
        <h3>Downloads</h3>
        <p>All subsequent folders and files are specified w.r.t.
        <a target='_blank' href="https://drive.google.com/drive/folders/1lz-RBwe4y_rt8r4v5bB0NHnj9_Hq_m5d?usp=sharing">this Google Drive root</a>.</p>

        <p>
        <!-- If you simply want to try our trained models, see "Pre-Trained Models." -->
        If you want to use our rendered/processed data, see "Data."
        If you want to render your own data, also see "Metadata."</p>

        <h4>Data</h4>
        <p>We release the images rendered from the synthetic scenes above and
        the real images (<code>vasedeck</code> and <code>pinecone</code> from NeRF)
        processed to be compatible with our model's data format:
        <table class="invisible">
            <tbody class="invisible">
                <tr class="invisible" width="50%">
                    <td>Rendered Images</td>
                    <td>Real Images</td>
                </tr>
                <tr class="invisible" width="50%">
                    <td><code>./rendered-images/</code></td>
                    <td><code>./real-images/</code></td>
                </tr>
        </table></p>

        <h4>Metadata</h4>
        <p>We release
        the four .blend scenes (modified from what NeRF released): <code>lego</code>, <code>hotdog</code>, <code>ficus</code>, and <code>drums</code>,
        the training/testing light probes used in the paper, and
        the training/validation/testing cameras (exactly the same as what NeRF released):
        <table class="invisible">
            <tbody class="invisible">
                <tr class="invisible" width="50%">
                    <td>Scenes</td>
                    <td>Light Probes</td>
                    <td>Cameras</td>
                </tr>
                <tr class="invisible" width="50%">
                    <td><code>./blender-scenes.zip</code></td>
                    <td><code>./light-probes.zip</code></td>
                    <td><code>./cameras.zip</code></td>
                </tr>
        </table></p>
    </div>

    <div class="card">
        <h3>All Video Results</h3>
        <p>Coming soon...</p>
    </div>

    <div class="copyright_card">
        Copyright &copy; 2021 Paper Authors. All Rights Reserved.
    </div>
</body>
</html>
